<div align="center">

# üîÆ QREC & MAREK

### Reinforcement Learning for Optimal Quantum State Discrimination

[![arXiv](https://img.shields.io/badge/arXiv-2404.10726-b31b1b.svg)](https://arxiv.org/abs/2404.10726)
[![arXiv](https://img.shields.io/badge/arXiv-2203.09807-b31b1b.svg)](https://arxiv.org/abs/2203.09807)
[![arXiv](https://img.shields.io/badge/arXiv-2001.10283-b31b1b.svg)](https://arxiv.org/abs/2001.10283)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

*Model-free calibration of quantum receivers through trial and error*

<img src="https://raw.githubusercontent.com/matibilkis/marek/master/Screenshot%202021-09-06%20at%2014-11-58%20Real-time%20calibration%20of%20coherent-state%20receivers%20Learning%20by%20trial%20and%20error%20-%20PhysRevRe%5B...%5D.png" alt="Kennedy Receiver Setup" width="600"/>

</div>

---

## üìñ Overview

This repository provides a comprehensive framework for implementing **reinforcement learning (RL)** techniques to achieve **optimal quantum state discrimination** over unknown channels. The codebase enables real-time calibration and optimization of quantum devices‚Äîparticularly **coherent-state receivers**‚Äîwithout requiring prior knowledge of system parameters.

### üéØ The Challenge

Quantum devices are particularly challenging to operate: their functionality relies on precisely tuning parameters, yet environmental conditions constantly shift, causing detuning. Traditional approaches require detailed modeling of environmental behavior, which is often computationally unaffordable, while direct parameter measurements introduce extra noise.

### üí° Our Solution

We frame quantum receiver calibration as a **reinforcement learning problem**, where an agent learns optimal discrimination strategies through trial and error‚Äîwithout any prior knowledge of experimental details.

---

## ‚ú® Key Features

<table>
<tr>
<td width="50%">

### ü§ñ Q-Learning Framework
- **Œµ-greedy exploration** with configurable parameters
- **Adaptive learning rates** (1/N decay or fixed)
- **Real-time Q-value updates** for optimal policy discovery
- Support for **change-point detection** scenarios

</td>
<td width="50%">

### üî¨ Quantum Physics Engine
- **Born rule** probability calculations
- **Coherent state** displacement operations
- **Kennedy receiver** simulation
- **Variable-loss optical channel** modeling

</td>
</tr>
<tr>
<td width="50%">

### üìä Analysis & Visualization
- Learning curve generation
- Q-function landscape plotting
- Noise sensitivity analysis
- Comparative performance metrics

</td>
<td width="50%">

### üîÑ Dynamic Calibration
- **Model-free** control loops
- Continuous parameter re-calibration
- Adaptation to **environmental drift**
- Optimal Œ≤ displacement learning

</td>
</tr>
</table>

---

## üèóÔ∏è Repository Structure

```
qrec/
‚îú‚îÄ‚îÄ üìÅ qrec/                      # Core module
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                  # Q-learning utilities, physics functions
‚îú‚îÄ‚îÄ üìÅ experiments/               # Experimental configurations
‚îÇ   ‚îú‚îÄ‚îÄ 0/                        # Full exploration (Œµ=1.0)
‚îÇ   ‚îú‚îÄ‚îÄ 1/                        # Low exploration, 1/N learning rate
‚îÇ   ‚îú‚îÄ‚îÄ 2/                        # Change-point: Œ± = 1.5 ‚Üí 0.25
‚îÇ   ‚îú‚îÄ‚îÄ 3/                        # Fixed lr = 0.005
‚îÇ   ‚îú‚îÄ‚îÄ 4/                        # Fixed lr = 0.05
‚îÇ   ‚îú‚îÄ‚îÄ 5/                        # Change-point with fixed lr (best)
‚îÇ   ‚îî‚îÄ‚îÄ 6/                        # Noise inspection
‚îú‚îÄ‚îÄ üìÅ paper/                     # Publication figures
‚îú‚îÄ‚îÄ üìÑ basic_inspection.py        # Error landscape visualization
‚îú‚îÄ‚îÄ üìÑ index_experiments          # Experiment documentation
‚îî‚îÄ‚îÄ üìÑ requirements.txt           # Dependencies
```

### Related Repository: [MAREK](https://github.com/matibilkis/marek)

```
marek/
‚îú‚îÄ‚îÄ üìÅ main_programs/             # Core RL algorithms
‚îú‚îÄ‚îÄ üìÅ dynamic_programming/       # DP optimization modules
‚îú‚îÄ‚îÄ üìÅ bounds_optimals_and_limits/# Theoretical bounds computation
‚îú‚îÄ‚îÄ üìÅ plotting_programs/         # Visualization tools
‚îú‚îÄ‚îÄ üìÅ appendix_A/                # Supplementary materials
‚îú‚îÄ‚îÄ üìÅ tests/                     # Validation suite
‚îú‚îÄ‚îÄ üìÑ agent.py                   # RL agent implementation
‚îú‚îÄ‚îÄ üìÑ environment.py             # Quantum channel simulation
‚îú‚îÄ‚îÄ üìÑ training.py                # Training loop
‚îî‚îÄ‚îÄ üìÑ basics.py                  # Core physics functions
```

---

## üßÆ Mathematical Framework

### Coherent State Discrimination

The goal is to discriminate between coherent states |¬±Œ±‚ü© using a Kennedy-like receiver with displacement Œ≤:

```
P(n|Œ±) = exp(-|Œ±|¬≤) ¬∑ Œ¥‚Çô‚ÇÄ + (1 - exp(-|Œ±|¬≤)) ¬∑ Œ¥‚Çô‚ÇÅ
```

The **success probability** for a given displacement Œ≤:

```
P‚Çõ(Œ≤) = ¬Ω Œ£‚Çô max_{s‚àà{-1,+1}} P(n | sŒ± + Œ≤)
```

### Q-Learning Update Rules

**Action-value updates:**
```
Q‚ÇÅ(Œ≤, n, g) ‚Üê Q‚ÇÅ(Œ≤, n, g) + Œ± ¬∑ [r - Q‚ÇÅ(Œ≤, n, g)]
Q‚ÇÄ(Œ≤) ‚Üê Q‚ÇÄ(Œ≤) + Œ± ¬∑ [max_g Q‚ÇÅ(Œ≤, n, g) - Q‚ÇÄ(Œ≤)]
```

**Policy (Œµ-greedy):**
```
œÄ(Œ≤) = { random      with probability Œµ
       { argmax Q‚ÇÄ   with probability 1-Œµ
```

---

## üöÄ Quick Start

### Installation

```bash
# Clone this repository
git clone https://github.com/matibilkis/qrec.git
cd qrec

# Install dependencies
pip install -r requirements.txt
```

### Basic Usage

```python
from qrec.utils import *

# Initialize Q-tables with 25 discretized Œ≤ values
betas_grid, [q0, q1, n0, n1] = define_q(nbetas=25)

# Find model-aware optimal (for comparison)
mmin, p_star, beta_star = model_aware_optimal(betas_grid, alpha=0.4)

# Run Q-learning episode
hidden_phase = np.random.choice([0, 1])  # Nature chooses ¬±Œ±
indb, beta = ep_greedy(q0, betas_grid, ep=0.01)  # Agent chooses Œ≤
n = give_outcome(hidden_phase, beta, alpha=0.4)  # Photon detection
indg, guess = ep_greedy(q1[indb, n, :], [0, 1], ep=0.01)  # Agent guesses
reward = give_reward(guess, hidden_phase)  # Success/failure
```

### Running Experiments

```bash
# Navigate to experiment directory
cd experiments/5

# Run change-point experiment with optimal parameters
python change_point.py
```

---

## üìà Results

The RL agent successfully learns near-optimal receiver configurations:

| Experiment | Configuration | Key Finding |
|------------|---------------|-------------|
| 0 | Œµ = 1.0 (full exploration) | Baseline uniform sampling |
| 1 | Œµ = 0.01, lr = 1/N | Convergent but slow adaptation |
| 2 | Change-point, lr = 1/N | **Cannot adapt** to Œ± changes |
| 3 | Œµ = 0.01, lr = 0.005 | Stable but slow learning |
| 4 | Œµ = 0.01, lr = 0.05 | Good balance |
| 5 | Change-point, lr = 0.05 | **Successful re-calibration** ‚úì |

> üí° **Key Insight:** Fixed learning rates enable adaptation to changing channel conditions, while decaying rates (1/N) lock the agent to initial configurations.

---

## üìö Scientific Deliverables

This framework has enabled the following peer-reviewed publications:

### 1Ô∏è‚É£ Automatic Re-calibration of Quantum Devices by RL

> **T. Crosta, L. Reb√≥n, F. Vilari√±o, J. M. Matera, M. Bilkis**  
> *arXiv:2404.10726 (2024)*

[![Paper](https://img.shields.io/badge/üìÑ_Read_Paper-arXiv:2404.10726-blue)](https://arxiv.org/abs/2404.10726)

Investigates RL techniques for developing model-free control loops for continuous recalibration of quantum device parameters, addressing challenges posed by environmental detuning. Demonstrated through numerical simulations of Kennedy receiver-based long-distance quantum communication.

---

### 2Ô∏è‚É£ RL Calibration on Variable-Loss Optical Channels

> **Reinforcement-learning calibration of coherent-state receivers on variable-loss optical channels**  
> *arXiv:2203.09807 (2022)*

[![Paper](https://img.shields.io/badge/üìÑ_Read_Paper-arXiv:2203.09807-blue)](https://arxiv.org/abs/2203.09807)

Studies calibration of quantum receivers for optical coherent states transmitted over channels with **variable transmissivity**. Demonstrates how RL optimizes error probabilities and adapts to changing channel conditions in real-time.

---

### 3Ô∏è‚É£ Real-Time Calibration: Learning by Trial and Error

> **M. Bilkis et al.**  
> *Physical Review Research (2021)*  
> *arXiv:2001.10283*

[![Paper](https://img.shields.io/badge/üìÑ_Read_Paper-arXiv:2001.10283-blue)](https://arxiv.org/abs/2001.10283)

The foundational work demonstrating that RL protocols enable agents to learn **near-optimal coherent-state receivers** composed of passive linear optics, photodetectors, and classical adaptive control‚Äîall through real-time experimentation.

---

## üîß Core API Reference

### Physics Functions

| Function | Description |
|----------|-------------|
| `p(alpha, n)` | Born rule probability P(n\|Œ±) |
| `Perr(beta, alpha)` | Error probability for displacement Œ≤ |
| `give_outcome(phase, beta, alpha)` | Sample photon detection outcome |
| `model_aware_optimal(betas, alpha)` | Compute theoretical optimum |

### Q-Learning Functions

| Function | Description |
|----------|-------------|
| `define_q(nbetas)` | Initialize Q-tables and counters |
| `ep_greedy(qvals, actions, ep)` | Œµ-greedy action selection |
| `greedy(arr)` | Greedy selection (ties broken randomly) |
| `give_reward(guess, phase)` | Binary reward function |
| `Psq(q0, q1, betas, alpha)` | Evaluate current policy |

---

## üì¶ Dependencies

```
numpy          # Numerical computations
matplotlib     # Visualization
scipy          # Optimization (minimize)
tqdm           # Progress bars
numba          # JIT compilation (optional)
pickle         # Experiment serialization
```

---

## ü§ù Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## üìú License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üì¨ Citation

If you use this code in your research, please cite:

```bibtex
@article{crosta2024automatic,
  title={Automatic re-calibration of quantum devices by reinforcement learning},
  author={Crosta, T. and Reb{\'o}n, L. and Vilari{\~n}o, F. and Matera, J. M. and Bilkis, M.},
  journal={arXiv preprint arXiv:2404.10726},
  year={2024}
}

@article{bilkis2022reinforcement,
  title={Reinforcement-learning calibration of coherent-state receivers on variable-loss optical channels},
  author={Bilkis, M. and others},
  journal={arXiv preprint arXiv:2203.09807},
  year={2022}
}

@article{bilkis2021realtime,
  title={Real-time calibration of coherent-state receivers: Learning by trial and error},
  author={Bilkis, M. and others},
  journal={Physical Review Research},
  year={2021}
}
```

---

<div align="center">

**Built with üíú for the quantum computing community**

[‚¨Ü Back to Top](#-qrec--marek)

</div>

