<div align="center">

# ğŸ”® QREC & MAREK

### Reinforcement Learning for Optimal Quantum State Discrimination

[![arXiv](https://img.shields.io/badge/arXiv-2404.10726-b31b1b.svg?style=for-the-badge)](https://arxiv.org/abs/2404.10726)

[![arXiv](https://img.shields.io/badge/arXiv-2203.09807-gray.svg)](https://arxiv.org/abs/2203.09807)
[![arXiv](https://img.shields.io/badge/arXiv-2001.10283-gray.svg)](https://arxiv.org/abs/2001.10283)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**Automatic Re-calibration of Quantum Devices by Reinforcement Learning**

*Model-free calibration of quantum receivers through trial and error*

<img src="https://raw.githubusercontent.com/matibilkis/marek/master/Screenshot%202021-09-06%20at%2014-11-58%20Real-time%20calibration%20of%20coherent-state%20receivers%20Learning%20by%20trial%20and%20error%20-%20PhysRevRe%5B...%5D.png" alt="Kennedy Receiver Setup" width="600"/>

</div>

---

## ğŸ“– Overview

This repository provides a comprehensive framework for implementing **reinforcement learning (RL)** techniques to achieve **optimal quantum state discrimination** over unknown channels. The codebase enables real-time calibration and optimization of quantum devicesâ€”particularly **coherent-state receivers**â€”without requiring prior knowledge of system parameters.

### ğŸ¯ The Challenge

Quantum devices are particularly challenging to operate: their functionality relies on precisely tuning parameters, yet environmental conditions constantly shift, causing detuning. Traditional approaches require detailed modeling of environmental behavior, which is often computationally unaffordable, while direct parameter measurements introduce extra noise.

### ğŸ’¡ Our Solution

We frame quantum receiver calibration as a **reinforcement learning problem**, where an agent learns optimal discrimination strategies through trial and errorâ€”without any prior knowledge of experimental details.

---

## âœ¨ Key Features

<table>
<tr>
<td width="50%">

### ğŸ¤– Q-Learning Framework
- **Îµ-greedy exploration** with configurable parameters
- **Adaptive learning rates** (1/N decay or fixed)
- **Real-time Q-value updates** for optimal policy discovery
- Support for **change-point detection** scenarios

</td>
<td width="50%">

### ğŸ”¬ Quantum Physics Engine
- **Born rule** probability calculations
- **Coherent state** displacement operations
- **Kennedy receiver** simulation
- **Variable-loss optical channel** modeling

</td>
</tr>
<tr>
<td width="50%">

### ğŸ“Š Analysis & Visualization
- Learning curve generation
- Q-function landscape plotting
- Noise sensitivity analysis
- Comparative performance metrics

</td>
<td width="50%">

### ğŸ”„ Dynamic Calibration
- **Model-free** control loops
- Continuous parameter re-calibration
- Adaptation to **environmental drift**
- Optimal Î² displacement learning

</td>
</tr>
</table>

---

## ğŸ—ï¸ Repository Structure

```
qrec/
â”œâ”€â”€ ğŸ“ qrec/                      # Core module
â”‚   â””â”€â”€ utils.py                  # Q-learning utilities, physics functions
â”œâ”€â”€ ğŸ“ experiments/               # Experimental configurations
â”‚   â”œâ”€â”€ 0/                        # Full exploration (Îµ=1.0)
â”‚   â”œâ”€â”€ 1/                        # Low exploration, 1/N learning rate
â”‚   â”œâ”€â”€ 2/                        # Change-point: Î± = 1.5 â†’ 0.25
â”‚   â”œâ”€â”€ 3/                        # Fixed lr = 0.005
â”‚   â”œâ”€â”€ 4/                        # Fixed lr = 0.05
â”‚   â”œâ”€â”€ 5/                        # Change-point with fixed lr (best)
â”‚   â””â”€â”€ 6/                        # Noise inspection
â”œâ”€â”€ ğŸ“ paper/                     # Publication figures
â”œâ”€â”€ ğŸ“„ basic_inspection.py        # Error landscape visualization
â”œâ”€â”€ ğŸ“„ index_experiments          # Experiment documentation
â””â”€â”€ ğŸ“„ requirements.txt           # Dependencies
```

### Related Repository: [MAREK](https://github.com/matibilkis/marek)

```
marek/
â”œâ”€â”€ ğŸ“ main_programs/             # Core RL algorithms
â”œâ”€â”€ ğŸ“ dynamic_programming/       # DP optimization modules
â”œâ”€â”€ ğŸ“ bounds_optimals_and_limits/# Theoretical bounds computation
â”œâ”€â”€ ğŸ“ plotting_programs/         # Visualization tools
â”œâ”€â”€ ğŸ“ appendix_A/                # Supplementary materials
â”œâ”€â”€ ğŸ“ tests/                     # Validation suite
â”œâ”€â”€ ğŸ“„ agent.py                   # RL agent implementation
â”œâ”€â”€ ğŸ“„ environment.py             # Quantum channel simulation
â”œâ”€â”€ ğŸ“„ training.py                # Training loop
â””â”€â”€ ğŸ“„ basics.py                  # Core physics functions
```

---

## ğŸ§® Mathematical Framework

### Coherent State Discrimination

The goal is to discriminate between coherent states |Â±Î±âŸ© using a Kennedy-like receiver with displacement Î²:

```
P(n|Î±) = exp(-|Î±|Â²) Â· Î´â‚™â‚€ + (1 - exp(-|Î±|Â²)) Â· Î´â‚™â‚
```

The **success probability** for a given displacement Î²:

```
Pâ‚›(Î²) = Â½ Î£â‚™ max_{sâˆˆ{-1,+1}} P(n | sÎ± + Î²)
```

### Q-Learning Update Rules

**Action-value updates:**
```
Qâ‚(Î², n, g) â† Qâ‚(Î², n, g) + Î± Â· [r - Qâ‚(Î², n, g)]
Qâ‚€(Î²) â† Qâ‚€(Î²) + Î± Â· [max_g Qâ‚(Î², n, g) - Qâ‚€(Î²)]
```

**Policy (Îµ-greedy):**
```
Ï€(Î²) = { random      with probability Îµ
       { argmax Qâ‚€   with probability 1-Îµ
```

---

## ğŸš€ Quick Start

### Installation

```bash
# Clone this repository
git clone https://github.com/matibilkis/qrec.git
cd qrec

# Install dependencies
pip install -r requirements.txt
```

### Basic Usage

```python
from qrec.utils import *

# Initialize Q-tables with 25 discretized Î² values
betas_grid, [q0, q1, n0, n1] = define_q(nbetas=25)

# Find model-aware optimal (for comparison)
mmin, p_star, beta_star = model_aware_optimal(betas_grid, alpha=0.4)

# Run Q-learning episode
hidden_phase = np.random.choice([0, 1])  # Nature chooses Â±Î±
indb, beta = ep_greedy(q0, betas_grid, ep=0.01)  # Agent chooses Î²
n = give_outcome(hidden_phase, beta, alpha=0.4)  # Photon detection
indg, guess = ep_greedy(q1[indb, n, :], [0, 1], ep=0.01)  # Agent guesses
reward = give_reward(guess, hidden_phase)  # Success/failure
```

### Running Experiments

```bash
# Navigate to experiment directory
cd experiments/5

# Run change-point experiment with optimal parameters
python change_point.py
```

---

## ğŸ“ˆ Results

The RL agent successfully learns near-optimal receiver configurations:

| Experiment | Configuration | Key Finding |
|------------|---------------|-------------|
| 0 | Îµ = 1.0 (full exploration) | Baseline uniform sampling |
| 1 | Îµ = 0.01, lr = 1/N | Convergent but slow adaptation |
| 2 | Change-point, lr = 1/N | **Cannot adapt** to Î± changes |
| 3 | Îµ = 0.01, lr = 0.005 | Stable but slow learning |
| 4 | Îµ = 0.01, lr = 0.05 | Good balance |
| 5 | Change-point, lr = 0.05 | **Successful re-calibration** âœ“ |

> ğŸ’¡ **Key Insight:** Fixed learning rates enable adaptation to changing channel conditions, while decaying rates (1/N) lock the agent to initial configurations.

---

## ğŸ“š Scientific Deliverables

<div align="center">

### ğŸŒŸ Main Publication

</div>

<table>
<tr>
<td>

## Automatic Re-calibration of Quantum Devices by Reinforcement Learning

**T. Crosta, L. RebÃ³n, F. VilariÃ±o, J. M. Matera, M. Bilkis**

[![Paper](https://img.shields.io/badge/ğŸ“„_Read_Paper-arXiv:2404.10726-b31b1b?style=for-the-badge)](https://arxiv.org/abs/2404.10726)

This work presents a **model-free reinforcement learning framework** for the continuous recalibration of quantum device parameters. We address the fundamental challenge of environmental detuning in quantum systemsâ€”where device functionality depends on precise parameter tuning, yet environmental conditions constantly shift.

**Key Contributions:**
- ğŸ”„ Model-free control loop for **continuous parameter recalibration**
- ğŸ¯ No prior knowledge of system dynamics required
- ğŸ“¡ Demonstrated on **Kennedy receiver-based long-distance quantum communication**
- âš¡ Real-time adaptation to environmental drift
- ğŸ“Š Numerical validation across multiple noise regimes

</td>
</tr>
</table>

---

### ğŸ“– Related Publications

<details>
<summary><b>Reinforcement-Learning Calibration on Variable-Loss Optical Channels</b> (2022)</summary>

> *arXiv:2203.09807*

[![Paper](https://img.shields.io/badge/ğŸ“„_Read_Paper-arXiv:2203.09807-blue)](https://arxiv.org/abs/2203.09807)

Studies calibration of quantum receivers for optical coherent states transmitted over channels with **variable transmissivity**. Demonstrates how RL optimizes error probabilities and adapts to changing channel conditions in real-time.

</details>

<details>
<summary><b>Real-Time Calibration of Coherent-State Receivers: Learning by Trial and Error</b> (2021)</summary>

> **M. Bilkis et al.** â€” *Physical Review Research*  
> *arXiv:2001.10283*

[![Paper](https://img.shields.io/badge/ğŸ“„_Read_Paper-arXiv:2001.10283-blue)](https://arxiv.org/abs/2001.10283)

The foundational work demonstrating that RL protocols enable agents to learn **near-optimal coherent-state receivers** composed of passive linear optics, photodetectors, and classical adaptive controlâ€”all through real-time experimentation.

</details>

---

## ğŸ”§ Core API Reference

### Physics Functions

| Function | Description |
|----------|-------------|
| `p(alpha, n)` | Born rule probability P(n\|Î±) |
| `Perr(beta, alpha)` | Error probability for displacement Î² |
| `give_outcome(phase, beta, alpha)` | Sample photon detection outcome |
| `model_aware_optimal(betas, alpha)` | Compute theoretical optimum |

### Q-Learning Functions

| Function | Description |
|----------|-------------|
| `define_q(nbetas)` | Initialize Q-tables and counters |
| `ep_greedy(qvals, actions, ep)` | Îµ-greedy action selection |
| `greedy(arr)` | Greedy selection (ties broken randomly) |
| `give_reward(guess, phase)` | Binary reward function |
| `Psq(q0, q1, betas, alpha)` | Evaluate current policy |

---

## ğŸ“¦ Dependencies

```
numpy          # Numerical computations
matplotlib     # Visualization
scipy          # Optimization (minimize)
tqdm           # Progress bars
numba          # JIT compilation (optional)
pickle         # Experiment serialization
```

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ“¬ Citation

If you use this code in your research, please cite our main paper:

```bibtex
@article{crosta2024automatic,
  title={Automatic re-calibration of quantum devices by reinforcement learning},
  author={Crosta, T. and Reb{\'o}n, L. and Vilari{\~n}o, F. and Matera, J. M. and Bilkis, M.},
  journal={arXiv preprint arXiv:2404.10726},
  year={2024}
}
```

<details>
<summary>Additional citations for related work</summary>

```bibtex
@article{bilkis2022reinforcement,
  title={Reinforcement-learning calibration of coherent-state receivers on variable-loss optical channels},
  author={Bilkis, M. and others},
  journal={arXiv preprint arXiv:2203.09807},
  year={2022}
}

@article{bilkis2021realtime,
  title={Real-time calibration of coherent-state receivers: Learning by trial and error},
  author={Bilkis, M. and others},
  journal={Physical Review Research},
  year={2021}
}
```

</details>

---

<div align="center">

**Built with ğŸ’œ for the quantum computing community**

[â¬† Back to Top](#-qrec--marek)

</div>

